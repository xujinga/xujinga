<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[HashMap源码解析]]></title>
    <url>%2F2019%2F05%2F28%2FHashMap%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[HashMap简介HashMap基于哈希表的 Map 接口的实现。此实现提供所有可选的映射操作，并允许使用 null 值和 null 键。（除了不同步和允许使用 null 之外，HashMap 类与 Hashtable 大致相同。）此类不保证映射的顺序，特别是它不保证该顺序恒久不变。值得注意的是HashMap不是线程安全的，如果想要线程安全的HashMap，可以通过Collections类的静态方法synchronizedMap获得线程安全的HashMap。Map map = Collections.synchronizedMap(new HashMap()); HashMap的数据结构HashMap的底层主要是基于数组和链表来实现的，它之所以有相当快的查询速度主要是因为它是通过计算散列码来决定存储的位置。HashMap中主要是通过key的hashCode来计算hash值的，只要hashCode相同，计算出来的hash值就一样。如果存储的对象对多了，就有可能不同的对象所算出来的hash值是相同的，这就出现了所谓的hash冲突。学过数据结构的同学都知道，解决hash冲突的方法有很多，HashMap底层是通过链表来解决hash冲突的。 下面一幅图，形象的反映出HashMap的数据结构：数组加链表实现 HashMap属性123456789101112131415161718192021222324252627//树化链表节点的阈值，当某个链表的长度大于或者等于这个长度，则扩大数组容量，或者数化链表static final int TREEIFY_THRESHOLD = 8; //初始容量，必须是2的倍数，默认是16 static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16 //最大所能容纳的key-value 个数 static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; //默认的加载因子 static final float DEFAULT_LOAD_FACTOR = 0.75f;//存储数据的Node数组，长度是2的幂。 transient Node&lt;K,V&gt;[] table; //keyset 方法要返回的结果 transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet; //map中保存的键值对的数量 transient int size; //hashmap 对象被修改的次数 transient int modCount; // 容量乘以装在因子所得结果，如果key-value的 数量等于该值，则调用resize方法，扩大容量，同时修改threshold的值。 int threshold; //装载因子 final float loadFactor; 构造方法默认构造方法默认构造方法将使用默认的加载因子（0.75）初始化。123public HashMap() &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted &#125; HashMap(int initialCapacity, float loadFactor)使用指定的初始容量和默认的加载因子初始化HashMap，这里需要注意的是，并不是你指定的初始容量是多少那么初始化之后的HashMap的容量就是多大，例如new HashMap(20,0.8); 那么实际的初始化容量是32，因为tableSizeFor（）方法会严格要求把初始化的容量是以2的次方数成长只能是16、32、64、128… 123456789101112public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal initial capacity: &quot; + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(&quot;Illegal load factor: &quot; + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity); &#125; 下面我们来看看tableSizeFor方法的实现： 123456789101112/** * 根据入参 返回2的指数 容量值 */ static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; &#125; HashMap(int initialCapacity)123public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR); &#125; HashMap(Map&lt;? extends K, ? extends V&gt; m)该方法是按照之前的hashMap的对象，重新深拷贝一份HashMap对象，使用的加载因子是默认的加载因子：0.75。 1234public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false); &#125; put方法执行逻辑：1）根据key计算当前Node的hash值，用于定位对象在HashMap数组的哪个节点。2）判断table有没有初始化，如果没有初始化，则调用resize（）方法为table初始化容量，以及threshold的值。3）根据hash值定位该key 对应的数组索引，如果对应的数组索引位置无值，则调用newNode（）方法，为该索引创建Node节点4）如果根据hash值定位的数组索引有Node，并且Node中的key和需要新增的key相等，则将对应的value值更新。5）如果在已有的table中根据hash找到Node，其中Node中的hash值和新增的hash相等，但是key值不相等的，那么创建新的Node，放到当前已存在的Node的链表尾部。 如果当前Node的长度大于8,则调用treeifyBin（）方法扩大table数组的容量，或者将当前索引的所有Node节点变成TreeNode节点，变成TreeNode节点的原因是由于TreeNode节点组成的链表索引元素会快很多。5）将当前的key-value 数量标识size自增，然后和threshold对比，如果大于threshold的值，则调用resize（）方法，扩大当前HashMap对象的存储容量。6）返回oldValue或者null。put 方法比较经常使用的方法，主要功能是为HashMap对象添加一个Node 节点，如果Node存在则更新Node里面的内容。 123public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true); &#125; put的主要的实现逻辑还是在putVal 实现的.下面我们来看看put主要实现逻辑： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162 /** * Implements Map.put and related methods * * @param key的hash值 * @param key值 * @param value值 * @param onlyIfAbsent如果是true，则不修改已存在的value值 * @param evict if false, the table is in creation mode. * @return 返回被修改的value，或者返回null。 */ final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) //如果是第一次调用，则会调用resize 初始化table 以及threshold n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) //如果对应的索引没有Node，则新建Node放到table里面。 tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) //如果hash值与已存在的hash相等，并且key相等，则准备更新对应Node的value e = p; else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; //如果hash值一致，但是key不一致，那么将新的key-value添加到已有的Node的最后面 for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // 当某个节点的链表长度大于8，则扩大table 数组的长度或者将当前节点链表变成树节点链表 treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) //hash值和key值相等的情况下，更新value值 e.value = value; //留给LinkedHashMap实现 afterNodeAccess(e); //返回旧的value return oldValue; &#125; &#125;//修改次数加1 ++modCount;//判断table的容量是否需要扩展 if (++size &gt; threshold) resize();//留给LinkedHashMap扩展 afterNodeInsertion(evict); return null; &#125; 上面调用到了一个resize方法， 我们来看看这个方法里面做了什么，实现逻辑如下：1）如果当前数组为空，则初始化当前数组2）如果当前table数组不为空，则将当前的table数组扩大两倍，同时将阈值（threshold）扩大两倍 数组长度和阈值扩大成两倍之后，将之前table数组中的值全部放到新的table中去 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384/** * 初始化，或者是扩展table 的容量。 * table的容量是按照2的指数增长的。 * 当扩大table 的容量的时候，元素的hash值以及位置可能发生变化。 */ final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table;//当前table 数组的长度 int oldCap = (oldTab == null) ? 0 : oldTab.length;//当前的阈值 int oldThr = threshold; int newCap, newThr = 0;//如果table数组已有值，则将其容量（size）和阈值（threshold）扩大两倍 if (oldCap &gt; 0) &#123; if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // 当第一次调用resize的时候会执行这个代码，初始化table容量以及阈值 newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125;//将新的阈值存储起来 threshold = newThr;//重新分配table 的容量 @SuppressWarnings(&#123;&quot;rawtypes&quot;,&quot;unchecked&quot;&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab;//将以前table中的值copy到新的table中去 if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab; &#125; 下面我们来看看treeifyBin方法的具体实现 123456789101112131415161718192021222324/** * 如果table长度太小，则扩大table 的数组长度 * 否则，将所有链表节点变成TreeNode，提高索引效率 */ final void treeifyBin(Node&lt;K,V&gt;[] tab, int hash) &#123; int n, index; Node&lt;K,V&gt; e; if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) resize(); else if ((e = tab[index = (n - 1) &amp; hash]) != null) &#123; TreeNode&lt;K,V&gt; hd = null, tl = null; do &#123; TreeNode&lt;K,V&gt; p = replacementTreeNode(e, null); if (tl == null) hd = p; else &#123; p.prev = tl; tl.next = p; &#125; tl = p; &#125; while ((e = e.next) != null); if ((tab[index] = hd) != null) hd.treeify(tab); &#125; &#125; get方法根据key的hash值和key，可以唯一确定一个value，下面我们来看看get方法执行的逻辑1）根据key计算hash值2）根据hash值和key 确定所需要返回的结果，如果不存在，则返回空。 1234public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value; &#125; 具体的实现在getNode方法实现 1234567891011121314151617181920212223242526/** * Implements Map.get and related methods * * @param key 的hash值 * @param key的值 * @return 返回由key和hash定位的Node，或者null */ final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; if (first.hash == hash &amp;&amp; // 如果索引到的第一个Node，key 和 hash值都和传递进来的参数相等，则返回该Node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) &#123; //如果索引到的第一个Node 不符合要求，循环变量它的下一个节点。 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null; &#125; containsKey方法containsKey方法实际也是调用getNode方法实现的，如果key对应的value不存在则返回false 123public boolean containsKey(Object key) &#123; return getNode(hash(key), key) != null; &#125; containsValue方法containsValue方法的话需要遍历对象所有的value，遇到value相等的，则返回true，具体实现如下 12345678910111213public boolean containsValue(Object value) &#123; Node&lt;K,V&gt;[] tab; V v; if ((tab = table) != null &amp;&amp; size &gt; 0) &#123; for (int i = 0; i &lt; tab.length; ++i) &#123; for (Node&lt;K,V&gt; e = tab[i]; e != null; e = e.next) &#123; if ((v = e.value) == value || (value != null &amp;&amp; value.equals(v))) return true; &#125; &#125; &#125; return false; &#125; remove方法执行逻辑：1）根据key得到key的hash值2）根据key 和hash值定位需要remove的Node3) 将Node从对应的链表移除，然后再将Node 前后的节点对接起来4）返回被移除 的Node5）key-value的数量减一，修改次数加一 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152 /** * Implements Map.remove and related methods * * @param key的hash值 * @param key值 * @param 需要remove 的value， * @param 为true时候，当value相等的时候才remove * @param 如果为false 的时候，不会移动其他节点。 * @return 返回被移除的Node，或者返回null */ final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) &#123; Node&lt;K,V&gt; node = null, e; K k; V v; if (p.hash == hash &amp;&amp; //如果定位到的第一个元素符合条件，则跳出if else ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; else if ((e = p.next) != null) &#123; if (p instanceof TreeNode) node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); else &#123; do &#123;//定位到的第一个Node元素不符合条件，则遍历其链表 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; node = e; break; &#125; p = e; &#125; while ((e = e.next) != null); &#125; &#125; //移除符合要求的节点，将链表重新连接起来 if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; if (node instanceof TreeNode) ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); else if (node == p) tab[index] = node.next; else p.next = node.next;//修改次数加1 ++modCount;//当前的key-value 对数减一 --size; afterNodeRemoval(node); return node; &#125; &#125; return null; replace方法replace(K key, V oldValue, V newValue)根据key和value定位到Node，然后将Node中的value用新value 替换，返回旧的value，否则返回空。 12345678910public boolean replace(K key, V oldValue, V newValue) &#123; Node&lt;K,V&gt; e; V v; if ((e = getNode(hash(key), key)) != null &amp;&amp; ((v = e.value) == oldValue || (v != null &amp;&amp; v.equals(oldValue)))) &#123; e.value = newValue; afterNodeAccess(e); return true; &#125; return false; &#125; replace(K key, V value)根据key定位到Node，然后将Node中的value 替换，返回旧的value，否则返回空 12345678910public V replace(K key, V value) &#123; Node&lt;K,V&gt; e; if ((e = getNode(hash(key), key)) != null) &#123; V oldValue = e.value; e.value = value; afterNodeAccess(e); return oldValue; &#125; return null;&#125; clear方法clear 方法将每个数组元素置空 123456789public void clear() &#123; Node&lt;K,V&gt;[] tab; modCount++; if ((tab = table) != null &amp;&amp; size &gt; 0) &#123; size = 0; for (int i = 0; i &lt; tab.length; ++i) tab[i] = null; &#125; &#125; hashCode() 和 equals(Object o)HashMap 自己没有重写equals和hashCode方法 是集成的它的父类AbstractMap的equals和hashCode equals(Object o)123456789101112131415161718192021222324252627282930313233343536public boolean equals(Object o) &#123; if (o == this) return true; if (!(o instanceof Map)) return false; Map&lt;?,?&gt; m = (Map&lt;?,?&gt;) o; if (m.size() != size()) return false; try &#123; //遍历map Iterator&lt;Entry&lt;K,V&gt;&gt; i = entrySet().iterator(); while (i.hasNext()) &#123; Entry&lt;K,V&gt; e = i.next(); K key = e.getKey(); V value = e.getValue(); //判断这个键对应的值是否为空 if (value == null) &#123; //为空的话 比较的值获取到的值不为空或者比较的值没有这个键 返回false if (!(m.get(key)==null &amp;&amp; m.containsKey(key))) return false; &#125; else &#123; //如果不为空的话 值和比较值获取到的值不同的话 返回false if (!value.equals(m.get(key))) return false; &#125; &#125; &#125; catch (ClassCastException unused) &#123; return false; &#125; catch (NullPointerException unused) &#123; return false; &#125; return true; &#125; hashCode()它的hashcode 是累加所有Entry&lt;K,V&gt;的hashcode()1234567public int hashCode() &#123; int h = 0; Iterator&lt;Entry&lt;K,V&gt;&gt; i = entrySet().iterator(); while (i.hasNext()) h += i.next().hashCode(); return h; &#125; 我们看看Entry&lt;K,V&gt;的hashcode()是怎么写的1234public final int hashCode() &#123;//key的hashCode 异或 value的hashcode return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; 所以往HashSet中放HashMap 只要键值一样就会被当作是同一个，便会去重。]]></content>
      <categories>
        <category>jdk源码</category>
      </categories>
      <tags>
        <tag>jdk源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[word转pdf实现在线预览]]></title>
    <url>%2F2019%2F03%2F29%2Fword%E8%BD%ACpdf%E5%AE%9E%E7%8E%B0%E5%9C%A8%E7%BA%BF%E9%A2%84%E8%A7%88%2F</url>
    <content type="text"><![CDATA[这几天在做的一个项目里面有个需求，一些文件需要在前端嵌入到页面预览，但是服务器存的文件有word,有pdf,前台没法处理word文件，就需要后台把word转成pdf输出给前台展示。我在网上搜了一下，有很多种方法，我这边使用的是openOffice的服务来做的转换。 1.下载安装openOffice进入OpenOffice下载对应版本的openOffice,并启动服务 win 启动方法在 program 目录下 双击soffice.exe linux 启动方法临时启动/opt/openoffice4/program/soffice -headless -accept=&quot;socket,host=127.0.0.1,port=2002;urp;&quot; -nofirststartwizard &amp;永久启动nohup /opt/openoffice4/program/soffice -headless -accept=&quot;socket,host=127.0.0.1,port=2002;urp;&quot; -nofirststartwizard &amp; 2.封装工具类安装jar包123456789101112131415161718192021222324252627282930&lt;dependency&gt; &lt;groupId&gt;org.openoffice&lt;/groupId&gt; &lt;artifactId&gt;jurt&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.openoffice&lt;/groupId&gt; &lt;artifactId&gt;ridl&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.openoffice&lt;/groupId&gt; &lt;artifactId&gt;juh&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.openoffice&lt;/groupId&gt; &lt;artifactId&gt;unoil&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.openoffice&lt;/groupId&gt; &lt;artifactId&gt;unoil&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.artofsolving&lt;/groupId&gt; &lt;artifactId&gt;jodconverter&lt;/artifactId&gt; &lt;version&gt;2.2.1&lt;/version&gt; &lt;/dependency&gt; 注意：jodconverter 2.2.1的版本不支持docx 2.2.2版本的中央库没有，文末会分享给大家 编写工具类1234567891011121314151617181920212223242526272829303132public class Word2Pdf &#123; // 将word格式的文件转换为pdf格式 //调用方法，传入原文档路径和目标文档路径即可完成转换 public static void Word2Pdf(String srcPath, String desPath) throws IOException &#123; // 源文件目录 File inputFile = new File(srcPath); if (!inputFile.exists()) &#123; log.error("源文件不存在"); return; &#125; // 输出文件目录 File outputFile = new File(desPath); if (!outputFile.getParentFile().exists()) &#123; outputFile.getParentFile().exists(); &#125; // 连接openoffice服务 OpenOfficeConnection connection = new SocketOpenOfficeConnection( "127.0.0.1", 8100); connection.connect(); log.debug("连接office服务"); // 转换word到pdf DocumentConverter converter = new OpenOfficeDocumentConverter( connection); converter.convert(inputFile, outputFile); // 关闭连接 connection.disconnect(); log.debug("转换完成"); &#125;&#125; 3.乱码问题解决乱码原因英文系统的Linux 内字体缺少，而对应需要转换的word中字体有多种不同类型字体，转换时无法对应，文末分享相关字体给大家 乱码解决 把字体文件夹放入到 /usr/share/fonts 刷新缓存：fc-cache 点击百度云获取字体和jar包 提取码:q4yh]]></content>
      <categories>
        <category>工具类</category>
      </categories>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用七牛云做图片上传]]></title>
    <url>%2F2019%2F03%2F27%2F%E4%BD%BF%E7%94%A8%E4%B8%83%E7%89%9B%E4%BA%91%E5%81%9A%E5%9B%BE%E7%89%87%E4%B8%8A%E4%BC%A0%2F</url>
    <content type="text"><![CDATA[1.如何使用七牛云？注册登录不多说，点击七牛云进行注册登录。 添加新的对象存储空间 我这里添加的存储空间交photo 记录一些后面用到的重要参数进入刚刚创建的存储空间–内容管理，记住外链默认域名 点击我的–个人中心–密钥管理，记住AK SK 2.后台使用七牛云上传图片pox引入依赖12345&lt;dependency&gt; &lt;groupId&gt;com.qiniu&lt;/groupId&gt; &lt;artifactId&gt;qiniu-java-sdk&lt;/artifactId&gt; &lt;version&gt;7.1.1&lt;/version&gt; &lt;/dependency&gt; 编写七牛云工具类根据自己的需求改写，这种是前端调用返回图片链接的方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class QiniuCloudUtil &#123; // 设置需要操作的账号的AK和SK private static final String ACCESS_KEY = "填写刚刚记下的AK"; private static final String SECRET_KEY = "填写刚刚记下的SK"; // 要上传的空间 private static final String bucketname = "自己创建的存储空间名字"; // 密钥 private static final Auth auth = Auth.create(ACCESS_KEY, SECRET_KEY); //存储位置 private static final String DOMAIN = "刚刚记录的外链"; //private static final String style = "自定义的图片样式"; public String getUpToken() &#123; return auth.uploadToken(bucketname, null, 3600, new StringMap().put("insertOnly", 1)); &#125; //base64方式上传 public String put64image(byte[] base64, String key) throws Exception &#123; String file64 = Base64.encodeToString(base64, 0); Integer l = base64.length; String url = "http://upload.qiniu.com/putb64/" + l + "/key/" + UrlSafeBase64.encodeToString(key); //非华东空间需要根据注意事项 1 修改上传域名 RequestBody rb = RequestBody.create(null, file64); Request request = new Request.Builder(). url(url). addHeader("Content-Type", "application/octet-stream") .addHeader("Authorization", "UpToken " + getUpToken()) .post(rb).build(); //System.out.println(request.headers()); OkHttpClient client = new OkHttpClient(); okhttp3.Response response = client.newCall(request).execute(); //sSystem.out.println(response); //如果不需要添加图片样式，使用以下方式 if (response.code() == 200) &#123; return DOMAIN + "/" + key; &#125; else &#123; throw new RuntimeException(response.message()); &#125; //return DOMAIN + key + "?" + style; &#125;&#125; 编写上传接口给前端调用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class UploadController &#123; //允许上传的图片类型 private final static List&lt;String&gt; ALLOW_EXT = Arrays.asList("jpg", "jpeg", "gif", "png", "bmp"); @PostMapping(value = "/uploadImg") @ApiOperation("图片上传") public String uploadImg(@RequestParam(value = "image") MultipartFile image) &#123; if (image.isEmpty()) &#123; return RequestResult.err("文件为空，请重新上传"); &#125; String extension = getExtension(image.getContentType()); if (extension == null || !ALLOW_EXT.contains(extension)) &#123; return RequestResult.err("不支持该文件类型"); &#125; try &#123; byte[] bytes = image.getBytes(); String imageName = UUID.randomUUID().toString(); QiniuCloudUtil qiniuUtil = new QiniuCloudUtil(); try &#123; //使用base64方式上传到七牛云 String url = qiniuUtil.put64image(bytes, imageName); return RequestResult.success("url", url); &#125; catch (Exception e) &#123; e.printStackTrace(); return RequestResult.err(e.getMessage()); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); return RequestResult.err(e.getMessage()); &#125; &#125; private String getExtension(String contentType) &#123; if (StringUtils.isNotBlank(contentType)) &#123; //通过imgContentType判断 int index = contentType.lastIndexOf("/"); if (index &lt; 0 || index + 1 &gt; contentType.length()) &#123; return null; &#125; else &#123; return contentType.substring(index + 1).toLowerCase(); &#125; &#125; return null; &#125;&#125; 3.postMan模拟前端调用接口返回上传后图片的链接 访问链接，就能看见刚刚上传的图片 打开七牛云刚刚自己创建的存储空间，点卡内容管理，也能看见自己刚刚上传的图片 这样的话使用七牛云做简单的图片上传功能就完成了，有一些进阶操作可以参考七牛云开发文档。]]></content>
      <categories>
        <category>工具类</category>
      </categories>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用hutool导出excel]]></title>
    <url>%2F2019%2F03%2F06%2F%E4%BD%BF%E7%94%A8hutool%E5%AF%BC%E5%87%BAexcel%2F</url>
    <content type="text"><![CDATA[Hutool是一个Java工具包，里面有很多非常方便的工具类，之前我们做一个excel导出特别麻烦，逻辑也非常绕，用Hutool几行代码就搞定了，非常方便。 1.安装maven方式在项目的pom.xml的dependencies中加入以下内容1234567891011121314151617&lt;!--hutool--&gt; &lt;dependency&gt; &lt;groupId&gt;cn.hutool&lt;/groupId&gt; &lt;artifactId&gt;hutool-all&lt;/artifactId&gt; &lt;version&gt;4.5.0&lt;/version&gt; &lt;/dependency&gt; &lt;!--excel相关--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.poi&lt;/groupId&gt; &lt;artifactId&gt;poi-ooxml&lt;/artifactId&gt; &lt;version&gt;3.17&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;xerces&lt;/groupId&gt; &lt;artifactId&gt;xercesImpl&lt;/artifactId&gt; &lt;version&gt;2.11.0&lt;/version&gt; &lt;/dependency&gt; 说明 poi-ooxml 版本需高于 3.17（别问我3.8版本为啥不行，因为3.17 &gt; 3.8 ） xercesImpl版本高于2.11.0 2.封装自己使用的工具类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class ExcelUtils &#123; /** * 写出Bean数据 * * @param response * @param list 输出到excel的列表 * @param map excel 列和 类的字段对应关系 如 key: name --&gt; value:名字 * @param title excel 标题 */ public static void exportExcel(HttpServletResponse response, List list, Map&lt;String, String&gt; map, String title) throws IOException &#123; //设置输出头 response.setHeader("Content-Disposition", "attachment;fileName=" + new String((new SimpleDateFormat("yyyyMMddHHmmss").format(new Date()) + ".xlsx").getBytes("UTF-8"))); // 通过工具类创建writer ExcelWriter writer = ExcelUtil.getWriter(true); //自定义标题别名 Set&lt;Map.Entry&lt;String, String&gt;&gt; entries = map.entrySet(); Iterator&lt;Map.Entry&lt;String, String&gt;&gt; iterator = entries.iterator(); while (iterator.hasNext()) &#123; Map.Entry&lt;String, String&gt; next = iterator.next(); writer.addHeaderAlias(next.getKey(), next.getValue()); &#125; // 合并单元格后的标题行，使用默认标题样式 writer.merge(map.size() - 1, title); // 一次性写出内容，使用默认样式，强制输出标题 writer.write(list, true); //out为OutputStream，需要写出到的目标流 writer.flush(response.getOutputStream()); // 关闭writer，释放内存 writer.close(); &#125; /** * 写出map数据 * * @param response * @param list 输出到excel的列表 * @param title excel 标题 */ public static void exportExcel(HttpServletResponse response, List&lt;Map&lt;String, Object&gt;&gt; list, String title) throws IOException &#123; //设置输出头 response.setHeader("Content-Disposition", "attachment;fileName=" + new String((new SimpleDateFormat("yyyyMMddHHmmss").format(new Date()) + ".xlsx").getBytes("UTF-8"))); // 通过工具类创建writer ExcelWriter writer = ExcelUtil.getWriter(true); // 合并单元格后的标题行，使用默认标题样式 writer.merge(list.size() - 1, title); // 一次性写出内容，使用默认样式，强制输出标题 writer.write(list, true); //out为OutputStream，需要写出到的目标流 writer.flush(response.getOutputStream()); // 关闭writer，释放内存 writer.close(); &#125; 第一个方法的list 里面装的是bean，map装的是bean中英文字段和中文汉字的对应关系。第二个方法list装的是map map里面是类似map.put(“姓名”,”张三”)，map.put(“年龄”,”14”) 几行代码就搞定了，是不是特别简单。我这个是以流的方式给前台，浏览器访问就直接下载了，还有写入文件中的方法，具体的去hutool官方文档，里面有详细介绍]]></content>
      <categories>
        <category>工具类</category>
      </categories>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[客服系统二]]></title>
    <url>%2F2018%2F11%2F02%2F%E5%AE%A2%E6%9C%8D%E7%B3%BB%E7%BB%9F%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[一、netty配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class NettyConfig&#123; private static final Logger logger = LoggerFactory.getLogger(NettyConfigImpl.class); private final ServerBootstrap bootstrap = new ServerBootstrap(); private EventLoopGroup parentGroup = new NioEventLoopGroup(); private EventLoopGroup childGroup = new NioEventLoopGroup(); private Class channelClass = NioServerSocketChannel.class; public void setHandler() &#123; validate(); bootstrap.group(parentGroup, childGroup); bootstrap.channel(channelClass); bootstrap.childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline pipeline = ch.pipeline(); //自带的粘包拆包 pipeline.addFirst(new LengthFieldBasedFrameDecoder(1024*1024*10,3,4,0,0,false)); //解码器 pipeline.addLast("ProtocolDecoder", new ProtocolDecoder()); //编码器 pipeline.addLast("ProtocolEncoder", new ProtocolEncoder()); //心跳六秒检测一次 pipeline.addLast("IdleStateHandler", new IdleStateHandler(6, 0, 0)); //业务逻辑处理 pipeline.addLast("AcceptorHandler", new AcceptorHandler()); &#125; &#125;).option(ChannelOption.SO_BACKLOG, 128) .childOption(ChannelOption.SO_KEEPALIVE, true); &#125; public void start(int port, boolean sync) &#123; ChannelFuture future = null; try &#123; future = bootstrap.bind(port).sync(); logger.info("服务器启动成功 监听端口(" + port + ")"); if (sync) &#123; future.channel().closeFuture().sync(); &#125; else &#123; future.channel().closeFuture(); &#125; logger.info("服务器关闭"); &#125; catch (InterruptedException e) &#123; logger.warn("Netty绑定异常", e); &#125; finally &#123; parentGroup.shutdownGracefully(); childGroup.shutdownGracefully(); &#125; &#125;&#125; 二、定义协议头1234567891011121314151617/** * 传输层协议头. * __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ * | | | | | | * 1 1 1 4 Uncertainty * |__ __ __ __|__ __ __ __|__ __ __ __|__ __ __ __ __|__ __ __ __ __ __ __ __ __| * | | | | | | * Sign Type Status Body Length Body Content * |__ __ __ __|__ __ __ __|__ __ __ __|__ __ __ __ __|__ __ __ __ __ __ __ __ __| * &lt;p&gt; * 协议头7个字节定长 * Sign // 消息标志，请求／响应／通知，byte类型 * Type // 消息类型，登录／发送消息等，byte类型 * Status // 响应状态，成功／失败，byte类型 * BodyLength // 协议体长度，int类型 * */ 三、定义传输载体1234567891011121314public class MessageHolder &#123; // 消息标志 private byte sign; // 消息类型 private byte type; // 响应状态 private byte status; // Json消息体 private String body; // 接收到消息的通道 private Channel channel; //省略get set &#125; 四、定义编解码器编码器 123456789101112131415161718192021public class ProtocolEncoder extends MessageToByteEncoder&lt;MessageHolder&gt; &#123; @Override protected void encode(ChannelHandlerContext ctx, MessageHolder msg, ByteBuf out) throws Exception &#123; String body = msg.getBody(); //body = filterEmoji(body); if (body == null) &#123; throw new NullParamsException("body == null"); &#125; // 编码 byte[] bytes = body.getBytes("utf-8"); /*if (msg.getType() != 22) &#123; System.err.println(msg); &#125;*/ out.writeByte(msg.getSign()) .writeByte(msg.getType()) .writeByte(msg.getStatus()) .writeInt(bytes.length) .writeBytes(bytes); &#125;&#125; 解码器 1234567891011121314151617181920212223242526272829303132333435363738394041public class ProtocolDecoder extends ByteToMessageDecoder &#123; private static final Logger logger = LoggerFactory.getLogger(ProtocolDecoder.class); @Override protected void decode(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object&gt; out) throws Exception &#123; if (in.readableBytes() &lt; ProtocolHeader.HEADER_LENGTH) &#123; // 数据包长度小于协议头长度 logger.error("数据包长度小于协议头长度"); return; &#125; //标记索引位置 in.markReaderIndex(); // 开始解码 byte sign = in.readByte(); byte type = in.readByte(); byte status = in.readByte(); // 确认消息体长度 int bodyLength = in.readInt(); if (in.readableBytes() != bodyLength) &#123; // 消息体长度不一致 logger.error("消息体长度不一致"); //还原索引位置 in.resetReaderIndex(); return; &#125; byte[] bytes = new byte[bodyLength]; in.readBytes(bytes); MessageHolder messageHolder = new MessageHolder(); messageHolder.setSign(sign); messageHolder.setType(type); messageHolder.setStatus(status); messageHolder.setBody(new String(bytes, "utf-8")); out.add(messageHolder); &#125;&#125; 五、定义业务逻辑处理的handler1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public class AcceptorHandler extends ChannelInboundHandlerAdapter &#123; private static final Logger logger = LoggerFactory.getLogger(AcceptorHandler.class); private final BlockingQueue&lt;MessageHolder&gt; taskQueue; public AcceptorHandler() &#123; taskQueue = TaskQueue.getQueue(); &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; if (msg instanceof MessageHolder) &#123; MessageHolder messageHolder = (MessageHolder) msg; // 指定Channel messageHolder.setChannel(ctx.channel()); //任务分发 dispatch(messageHolder); &#125; else &#123; throw new IllegalArgumentException("msg is not instance of MessageHolder"); &#125; &#125; public static void dispatch(MessageHolder messageHolder) &#123; try &#123; if (messageHolder.getSign() != ProtocolHeader.REQUEST) &#123; // 请求错误 response(messageHolder.getChannel(), messageHolder.getType(), ProtocolHeader.REQUEST_ERROR); log.error("请求错误"); return; &#125; switch (messageHolder.getType()) &#123; // 登录 case ProtocolHeader.LOGIN: Account aLogin = JSON.parseObject(messageHolder.getBody(), Account.class); new Login(aLogin, messageHolder.getChannel()).deal(); break; // 登出 case ProtocolHeader.LOGOUT: Account aLogout = JSON.parseObject(messageHolder.getBody(), Account.class); new Logout(aLogout, messageHolder.getChannel()).deal(); break; // 微信个人消息 case ProtocolHeader.WECHAT_PERSON_MESSAGE: try &#123; Message wechatMessage = JSON.parseObject(messageHolder.getBody(), Message.class); new WechatPersonMessage(wechatMessage, messageHolder.getChannel()).deal(); &#125;catch (Exception e)&#123; log.error(messageHolder.getBody()); &#125; break; // 断线重连 case ProtocolHeader.RECONN: Account reconn = JSON.parseObject(messageHolder.getBody(), Account.class); new Reconn(reconn, messageHolder.getChannel()).deal(); break; // 请求错误 default: response(messageHolder.getChannel(), messageHolder.getType(), ProtocolHeader.REQUEST_ERROR); break; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); response(messageHolder.getChannel(), messageHolder.getType(), ProtocolHeader.SERVER_ERROR); &#125; // 释放buffer ReferenceCountUtil.release(messageHolder); &#125;&#125; 六、定义心跳handler1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283public class HeartbeatHandler extends ChannelInboundHandlerAdapter &#123; private static final Logger logger = LoggerFactory.getLogger(HeartbeatHandler.class); public static AtomicBoolean isLogout = new AtomicBoolean(false); private Channel channel; private String username; // 丢失的心跳数 private static int counter = 0; public HeartbeatHandler(Channel channel) &#123; this.channel = channel; &#125; @Override public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception &#123; if (evt instanceof IdleStateEvent) &#123; if (username == null) &#123; username = ConnPool.query(channel); &#125; // 心跳丢失 counter++; if (counter &gt; 4) &#123; // 心跳丢失数达到5个，主动断开连接 ctx.channel().close(); &#125; logger.info(username + " 丢失" + counter + "个心跳包"); &#125; &#125; @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123; ConnPool.remove(username); if (isLogout.get()) &#123; isLogout.set(false); logger.info(username + " 退出登录"); &#125; else &#123; logger.info(username + " 与服务器断开连接"); &#125; &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; if (msg instanceof MessageHolder) &#123; MessageHolder messageHolder = (MessageHolder) msg; if (messageHolder.getType() == ProtocolHeader.HEARTBEAT) &#123; if (username == null) &#123; username = ConnPool.query(channel); &#125; // 心跳丢失清零 counter = 0; //logger.info(username + " 收到心跳包"); Set&lt;String&gt; keys = RedisUtil.keys("im_" + username + "_*"); Map&lt;String, Integer&gt; map = new HashMap(); map.put("receptionNum", keys.size()); sendResponse(channel, ProtocolHeader.SUCCESS, ProtocolHeader.RECEPTION_NUM, JSON.toJSONString(map)); ReferenceCountUtil.release(msg); &#125; else &#123; ctx.fireChannelRead(msg); &#125; &#125; &#125; /** * 服务器响应 */ @SuppressWarnings("all") private void sendResponse(Channel channel, byte status, byte type, String body) &#123; MessageHolder messageHolder = new MessageHolder(); messageHolder.setSign(ProtocolHeader.RESPONSE); messageHolder.setType(type); messageHolder.setStatus(status); messageHolder.setBody(body); channel.writeAndFlush(messageHolder); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; sendResponse(ctx.channel(), ProtocolHeader.SERVER_ERROR, ProtocolHeader.EXCEPTION, "服务器异常"); logger.error(cause.getMessage()); &#125;&#125;]]></content>
      <categories>
        <category>客服系统</category>
      </categories>
      <tags>
        <tag>im</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用javaMail做异常提醒]]></title>
    <url>%2F2018%2F11%2F02%2F%E4%BD%BF%E7%94%A8javaMail%E5%81%9A%E5%BC%82%E5%B8%B8%E6%8F%90%E9%86%92%2F</url>
    <content type="text"><![CDATA[一、异常收集拦截所有前台得到的异常 并写入到.txt文件中1234567891011121314151617181920212223242526272829303132333435363738394041@Slf4j@ResponseBody@ControllerAdvicepublic class GlobalExceptionHandler &#123; /** * 所有异常报错 * * @param request * @param exception * @return * @throws Exception */ @ExceptionHandler(value = Exception.class) public String allExceptionHandler(HttpServletRequest request, Exception exception) throws Exception &#123; //将异常写入服务器中 String today = DateUtil.today(); String path = ResourceFileUtil.getPath("exception"); File parentDir = new File(path); if (!parentDir.exists()) &#123; parentDir.mkdirs(); &#125; FileWriter writer = null; PrintWriter printWriter = null; try &#123; String fileName = path + today + ".txt"; writer = new FileWriter(fileName, true); printWriter = new PrintWriter(writer, true); //异常的信息和原因一致 视为一个异常，同一种异常只写一次就行了 String key = "exception_" + today + "_" + exception.getMessage() + "_" + exception.getCause(); if (RedisUtil.setIfAbsent(key, true, 24, TimeUnit.HOURS)) &#123; exception.printStackTrace(printWriter); exception.printStackTrace(); &#125; &#125; finally &#123; writer.close(); printWriter.close(); &#125; return RequestResult.err("服务器忙"); &#125;&#125; 二、集成javaMail pom.xml 12345&lt;!-- 支持发送邮件 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-mail&lt;/artifactId&gt; &lt;/dependency&gt; 编写工具类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182public class EmailUtil &#123; private static String smtp = "smtp";//协议 private static String host = "smtp.qq.com";//使用什么服务器发送 aliyun qq 163等 private static String sendPort = "465";//发送的端口号 默认是25 一般服务器 厂商把25端口禁用了 private static String userName = "****@qq.com";//账号 private static String userPwd = "****";//授权码 private static String nickNmae = "****";//发送是的昵称 /** * 邮件发送的方法 * * @param to 收件人 * @param subject 主题 * @param content 内容 * @return 成功或失败 */ public static boolean send(String to, String subject, String content, String fileName) &#123; // 第一步：创建Session Properties props = new Properties(); // 指定邮件的传输协议，smtp(Simple Mail Transfer Protocol 简单的邮件传输协议) props.put("mail.smtp.auth", "true"); props.put("mail.transport.protocol", smtp); // 指定邮件发送服务器服务器 "smtp.qq.com" props.put("mail.smtp.host", host); props.put("mail.smtp.starttls.enable", "true"); props.put("mail.smtp.socketFactory.fallback", "false"); props.put("mail.smtp.socketFactory.port", sendPort); props.put("mail.smtp.socketFactory.class", "javax.net.ssl.SSLSocketFactory"); props.put("mail.smtp.port", sendPort); Session session = Session.getDefaultInstance(props); // 开启调试模式 session.setDebug(true); try &#123; // 第二步：获取邮件发送对象 Transport transport = session.getTransport(); // 连接邮件服务器，链接您的163、sina邮箱，用户名（不带@163.com，登录邮箱的邮箱账号，不是邮箱地址）、密码 transport.connect(userName, userPwd); Address toAddress = new InternetAddress(to); // 第三步：创建邮件消息体 MimeMessage message = new MimeMessage(session); message.setFrom(new InternetAddress(nickNmae + " &lt;" + userName + "&gt;")); // 邮件的主题 message.setSubject(subject); //收件人 message.addRecipient(Message.RecipientType.TO, toAddress); // 邮件的内容 Multipart mp = new MimeMultipart(); MimeBodyPart mbpContent = new MimeBodyPart(); mbpContent.setText(content); mp.addBodyPart(mbpContent); /* 往邮件中添加附件 */ File file = new File(fileName); if (file.exists()) &#123; MimeBodyPart mbpFile = new MimeBodyPart(); FileDataSource fds = new FileDataSource(fileName); mbpFile.setDataHandler(new DataHandler(fds)); mbpFile.setFileName(fds.getName()); mp.addBodyPart(mbpFile); &#125; message.setContent(mp); // 邮件发送时间 message.setSentDate(new Date()); // 第四步：发送邮件 // 第一个参数：邮件的消息体 // 第二个参数：邮件的接收人，多个接收人用逗号隔开（test1@163.com,test2@sina.com） transport.sendMessage(message, InternetAddress.parse(to)); return true; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return false; &#125;&#125; 三、定时发送邮件使用定时任务每天上午九点向开发者邮箱发送邮件12345678910111213141516171819202122232425@Component@EnableSchedulingpublic class MobileTask &#123; /** * 每日上午九点执行 * 发送异常邮件 */ @Scheduled(cron = "0 0 9 * * ?") // 每日上午九点执行 public void autoSendExceptionEmail() &#123; //发送邮件 String yesterday = DateUtil.yesterday().toString("yyyy-MM-dd"); String path = ResourceFileUtil.getPath("exception"); String fileName = path + yesterday + ".txt"; EmailUtil.send("****@qq.com", yesterday + "异常提醒邮件", yesterday + "异常附件，请查收(若无附件说明昨日没有产生异常)", fileName); //删除昨日所有的键 String keyPatten = "exception_" + yesterday + "*"; Set&lt;String&gt; exceptionKeys = RedisUtil.keys(keyPatten); Iterator&lt;String&gt; iterator = exceptionKeys.iterator(); while (iterator.hasNext()) &#123; String next = iterator.next(); RedisUtil.del(next); &#125; &#125;&#125;]]></content>
      <categories>
        <category>工具类</category>
      </categories>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用solr优化商品搜索效率]]></title>
    <url>%2F2018%2F11%2F02%2F%E4%BD%BF%E7%94%A8solr%E4%BC%98%E5%8C%96%E5%95%86%E5%93%81%E6%90%9C%E7%B4%A2%E6%95%88%E7%8E%87%2F</url>
    <content type="text"><![CDATA[一、前言我们的一个电商项目，之前前台使用的搜索接口是通过商品名称或者编号去数据库检索，返回给前台一系列符合的商品集合，能基本完成搜索的功能，但是当并发量高的的时候会对数据库造成不小的压力，而且查询速率也不是很好，所以我这周接到了这个任务，优化这个搜索功能，尝试使用solr去解决。 二、solr介绍什么是solr Solr是一个独立的企业级搜索应用服务器，它对外提供类似于Web-service的API接口。用户可以通过http请求，向搜索引擎服务器提交一定格式的XML文件，生成索引；也可以通过Http Get操作提出查找请求，并得到XML格式的返回结果。 为什么使用solr solr是将整个索引操作功能封装好了的搜索引擎系统(企业级搜索引擎产品) solr可以部署到单独的服务器上(WEB服务)，它可以提供服务，我们的业务系统就只要发送请求，接收响应即可，降低了业务系统的负载 solr部署在专门的服务器上，它的索引库就不会受业务系统服务器存储空间的限制 solr支持分布式集群，索引服务的容量和能力可以线性扩展 solr的工作机制 solr就是在lucene工具包的基础之上进行了封装，而且是以web服务的形式对外提供索引功能 业务系统需要使用到索引的功能（建索引，查索引）时，只要发出http请求，并将返回数据进行解析即可 Solr 是Apache下的一个顶级开源项目，采用Java开发，它是基于Lucene的全文搜索服务器。Solr提供了比Lucene更为丰富的查询语言，同时实现了可配置、可扩展，并对索引、搜索性能进行了优化。 Solr可以独立运行，运行在Jetty、Tomcat等这些Servlet容器中，Solr 索引的实现方法很简单，用 POST 方法向 Solr 服务器发送一个描述 Field 及其内容的 XML 文档，Solr根据xml文档添加、删除、更新索引 。Solr 搜索只需要发送 HTTP GET 请求，然后对 Solr 返回Xml、json等格式的查询结果进行解析，组织页面布局。Solr不提供构建UI的功能，Solr提供了一个管理界面，通过管理界面可以查询Solr的配置和运行情况。 三、solr的使用安装Apache Solr下载最新版本的Apache Solr 点击进行下载，下载进行解压，目录结构如图所示: 进入bin目录，使用命令行输入solr start在浏览器打开 http://localhost:8983/进入solr 管理界面这样solr 的安装就完成啦 solr 启动、停止、重启命令solr start -p 端口号solr stop -allsolr restart -p 端口号 配置Apache Solr创建core实例 core简介：简单说core就是solr的一个实例，一个solr服务下可以有多个core，每个core下都有自己的索引库和与之相应的配置文件，所以在操作solr创建索引之前要创建一个core，因为索引都存在core下面 core创建：core的创建方式有很多种一下列出两种比较方便的。 在bin目录下执行solr create –c name，创建一个core，默认创建出来的位置如下图 第二种方式是直接使用AdminUI页面创建一个core，如下图使用第二种方式会出现一个问题需要在将目录\solr-7.5.0\server\solr\configsets\_default目录下的 conf 拷贝到刚刚新建的core下面 重启solr服务就好了 配置schema schema简介： schema是用来告诉solr如何建立索引的，他的配置围绕着一个schema配置文件，这个配置文件决定着solr如何建立索引，每个字段的数据类型，分词方式等，老版本的schema配置文件的名字叫做schema.xml他的配置方式就是手工编辑，但是现在新版本的schema配置文件的名字叫做managed-schema，他的配置方式不再是用手工编辑而是使用schemaAPI来配置，官方给出的解释是使用schemaAPI修改managed-schema内容后不需要重新加载core或者重启solr更适合在生产环境下维护，如果使用手工编辑的方式更改配置不进行重加载core有可能会造成配置丢失，配置文件所在的路径如下图：直接贴上我的代码吧 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;schema name=&quot;example-DIH-db&quot; version=&quot;1.6&quot;&gt; &lt;!--自己查询的一些数据--&gt; &lt;field name=&quot;id&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot; required=&quot;true&quot; multiValued=&quot;false&quot; /&gt; &lt;field name=&quot;userId&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&quot;spuNo&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&quot;categoryId&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&quot;name&quot; type=&quot;text_ik&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&quot;type&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&quot;labels&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&quot;price&quot; type=&quot;double&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&quot;fixedPrice&quot; type=&quot;double&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&quot;minPrice&quot; type=&quot;double&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&quot;maxPrice&quot; type=&quot;double&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&quot;pics&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&quot;initialSales&quot; type=&quot;int&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&quot;realSales&quot; type=&quot;int&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&quot;onSale&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&quot;onSaleTime&quot; type=&quot;date&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&quot;isMul&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&quot;expressTempId&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&quot;stock&quot; type=&quot;int&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&quot;stockType&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&quot;priority&quot; type=&quot;int&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&quot;fatherCategoryId&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&quot;fromLibId&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&quot;vipPriceEnable&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&quot;promotionType&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&quot;createTime&quot; type=&quot;date&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&quot;lastUpdateTime&quot; type=&quot;date&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&quot;isDel&quot; type=&quot;boolean&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;!--solr自带的数据--&gt; &lt;field name=&quot;_version_&quot; type=&quot;long&quot; indexed=&quot;false&quot; stored=&quot;false&quot;/&gt; &lt;field name=&quot;text&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;false&quot; multiValued=&quot;true&quot;/&gt; &lt;field name=&quot;_root_&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;false&quot; docValues=&quot;false&quot; /&gt; &lt;!--主键--&gt; &lt;uniqueKey&gt;id&lt;/uniqueKey&gt; &lt;!--基本的一些数据类型 solr不支持数据库的bigDecimal 和tinyint 用double和boolean代替--&gt; &lt;fieldType name=&quot;string&quot; class=&quot;solr.StrField&quot; sortMissingLast=&quot;true&quot; /&gt; &lt;fieldType name=&quot;int&quot; class=&quot;solr.TrieIntField&quot; docValues=&quot;true&quot; precisionStep=&quot;0&quot; positionIncrementGap=&quot;0&quot;/&gt; &lt;fieldType name=&quot;long&quot; class=&quot;solr.TrieLongField&quot; docValues=&quot;true&quot; precisionStep=&quot;0&quot; positionIncrementGap=&quot;0&quot;/&gt; &lt;fieldType name=&quot;date&quot; class=&quot;solr.TrieDateField&quot; docValues=&quot;true&quot; precisionStep=&quot;0&quot; positionIncrementGap=&quot;0&quot;/&gt; &lt;fieldType name=&quot;double&quot; class=&quot;solr.TrieDoubleField&quot; docValues=&quot;true&quot; precisionStep=&quot;0&quot; positionIncrementGap=&quot;0&quot;/&gt; &lt;fieldType name=&quot;boolean&quot; class=&quot;solr.BoolField&quot; omitNorms=&quot;true&quot;/&gt; &lt;!-- ik分词器 --&gt; &lt;fieldType name=&quot;text_ik&quot; class=&quot;solr.TextField&quot;&gt; &lt;analyzer type=&quot;index&quot;&gt; &lt;tokenizer class=&quot;org.wltea.analyzer.lucene.IKTokenizerFactory&quot; useSmart=&quot;false&quot; conf=&quot;ik.conf&quot;/&gt; &lt;filter class=&quot;solr.LowerCaseFilterFactory&quot;/&gt; &lt;/analyzer&gt; &lt;analyzer type=&quot;query&quot;&gt; &lt;tokenizer class=&quot;org.wltea.analyzer.lucene.IKTokenizerFactory&quot; useSmart=&quot;true&quot; conf=&quot;ik.conf&quot;/&gt; &lt;filter class=&quot;solr.LowerCaseFilterFactory&quot;/&gt; &lt;/analyzer&gt; &lt;/fieldType&gt;&lt;/schema&gt; 配置IK Analyzer中文分词器先下载 IK Analyzer中文分词器架包及相关配置文件。 准备IK中文分词器环境 把IK中文分词器架包复制到solr项目的WEB-INF/lib目录下 把IK分词器相关配置文件复制到solr项目WEB-INF/classes/ 目录下 (没有自己新建) 配置solrconfig配置搜索建议，在solrconfig.xml找到suggest123456789101112131415161718192021222324252627282930&lt;searchComponent class=&quot;solr.SpellCheckComponent&quot; name=&quot;suggest&quot;&gt; &lt;str name=&quot;queryAnalyzerFieldType&quot;&gt;string&lt;/str&gt; &lt;lst name=&quot;spellchecker&quot;&gt; &lt;str name=&quot;name&quot;&gt;suggest&lt;/str&gt; &lt;str name=&quot;classname&quot;&gt;org.apache.solr.spelling.suggest.Suggester&lt;/str&gt; &lt;str name=&quot;lookupImpl&quot;&gt;org.apache.solr.spelling.suggest.tst.TSTLookup&lt;/str&gt; &lt;!--建议字段名--&gt; &lt;str name=&quot;field&quot;&gt;name&lt;/str&gt; &lt;!-- the indexed field to derive suggestions from --&gt; &lt;float name=&quot;threshold&quot;&gt;0.0001&lt;/float&gt; &lt;str name=&quot;spellcheckIndexDir&quot;&gt;spellchecker&lt;/str&gt; &lt;str name=&quot;comparatorClass&quot;&gt;freq&lt;/str&gt; &lt;str name=&quot;buildOnOptimize&quot;&gt;true&lt;/str&gt; &lt;!--&lt;str name=&quot;buildOnCommit&quot;&gt;true&lt;/str&gt;--&gt; &lt;/lst&gt; &lt;/searchComponent&gt; &lt;requestHandler class=&quot;org.apache.solr.handler.component.SearchHandler&quot; name=&quot;/suggest&quot;&gt; &lt;lst name=&quot;defaults&quot;&gt; &lt;str name=&quot;spellcheck&quot;&gt;true&lt;/str&gt; &lt;str name=&quot;spellcheck.dictionary&quot;&gt;suggest&lt;/str&gt; &lt;str name=&quot;spellcheck.onlyMorePopular&quot;&gt;true&lt;/str&gt; &lt;str name=&quot;spellcheck.extendedResults&quot;&gt;false&lt;/str&gt; &lt;str name=&quot;spellcheck.count&quot;&gt;10&lt;/str&gt; &lt;str name=&quot;spellcheck.collate&quot;&gt;true&lt;/str&gt; &lt;/lst&gt; &lt;arr name=&quot;components&quot;&gt; &lt;str&gt;suggest&lt;/str&gt; &lt;/arr&gt; &lt;/requestHandler&gt; 数据库数据导入 修改soreconfig.xml在soreconfig.xml的上面添加如下代码： 12345&lt;requestHandler name=&quot;/dataimport&quot; class=&quot;solr.DataImportHandler&quot;&gt; &lt;lst name=&quot;defaults&quot;&gt; &lt;str name=&quot;config&quot;&gt;db-data-config.xml&lt;/str&gt; &lt;/lst&gt; &lt;/requestHandler&gt; 在同级目录下创建data-config.xml文件，然后配置数据库相关属性注意：这里面配置的属性需要和上文中 schema中配置的一样 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;dataConfig&gt; &lt;dataSource type=&quot;JdbcDataSource&quot; driver=&quot;com.mysql.jdbc.Driver&quot; url=&quot;jdbc:mysql://127.0.0.1:3306/saas?useUnicode=true&amp;amp;characterEncoding=utf8&amp;amp;useSSL=false&quot; user=&quot;root&quot; password=&quot;123456&quot;/&gt; &lt;document&gt; &lt;entity dataSource=&quot;JdbcDataSource&quot; name=&quot;mall_product&quot; query=&quot;select mp.*,mo.promotionType from mall_product mp LEFT JOIN mall_promotion mo ON mp.id = mo.productId AND mo.`status` = 0 where mp.isDel = 0; &quot; pk=&quot;id&quot;&gt; &lt;field column=&quot;id&quot; name=&quot;id&quot;/&gt; &lt;field column=&quot;userId&quot; name=&quot;userId&quot;/&gt; &lt;field column=&quot;spuNo&quot; name=&quot;spuNo&quot;/&gt; &lt;field column=&quot;categoryId&quot; name=&quot;categoryId&quot;/&gt; &lt;field column=&quot;name&quot; name=&quot;name&quot;/&gt; &lt;field column=&quot;type&quot; name=&quot;type&quot;/&gt; &lt;field column=&quot;labels&quot; name=&quot;labels&quot;/&gt; &lt;field column=&quot;price&quot; name=&quot;price&quot;/&gt; &lt;field column=&quot;fixedPrice&quot; name=&quot;fixedPrice&quot;/&gt; &lt;field column=&quot;minPrice&quot; name=&quot;minPrice&quot;/&gt; &lt;field column=&quot;maxPrice&quot; name=&quot;maxPrice&quot;/&gt; &lt;field column=&quot;pics&quot; name=&quot;pics&quot;/&gt; &lt;field column=&quot;initialSales&quot; name=&quot;initialSales&quot;/&gt; &lt;field column=&quot;realSales&quot; name=&quot;realSales&quot;/&gt; &lt;field column=&quot;onSale&quot; name=&quot;onSale&quot;/&gt; &lt;field column=&quot;onSaleTime&quot; name=&quot;onSaleTime&quot;/&gt; &lt;field column=&quot;isMul&quot; name=&quot;isMul&quot;/&gt; &lt;field column=&quot;expressTempId&quot; name=&quot;expressTempId&quot;/&gt; &lt;field column=&quot;stock&quot; name=&quot;stock&quot;/&gt; &lt;field column=&quot;stockType&quot; name=&quot;stockType&quot;/&gt; &lt;field column=&quot;priority&quot; name=&quot;priority&quot;/&gt; &lt;field column=&quot;fatherCategoryId&quot; name=&quot;fatherCategoryId&quot;/&gt; &lt;field column=&quot;fromLibId&quot; name=&quot;fromLibId&quot;/&gt; &lt;field column=&quot;vipPriceEnable&quot; name=&quot;vipPriceEnable&quot;/&gt; &lt;field column=&quot;promotionType&quot; name=&quot;promotionType&quot;/&gt; &lt;field column=&quot;createTime&quot; name=&quot;createTime&quot;/&gt; &lt;field column=&quot;lastUpdateTime&quot; name=&quot;lastUpdateTime&quot;/&gt; &lt;field column=&quot;isDel&quot; name=&quot;isDel&quot;/&gt; &lt;/entity&gt; &lt;/document&gt;&lt;/dataConfig&gt; 拷贝jar拷贝solr-7.5.0\dist路径下的solr-dataimporthandler-7.5.0.jar，solr-dataimporthandler-extras-7.5.0.jar 到 solr-7.5.0\server\solr-webapp\webapp\WEB-INF\lib目录下同时拷贝mysql-connector-java-5.1.40.jar链接jar到该目录下 数据导入重启solr 进入管理页面然后就能看见你要的数据了 四、springboot集成solr 引入jar包 1234&lt;dependency&gt;&lt;groupId&gt;org.springframework.data&lt;/groupId&gt;&lt;artifactId&gt;spring-data-solr&lt;/artifactId&gt;&lt;/dependency&gt; 配置文件 1234spring: data: solr: host: http://127.0.0.1:8983/solr 相关代码 @Service public class ProductSolrServiceImpl implements ProductSolrService { @Resource private PromotionService promotionService; @Resource private SolrClient solrClient; @Override public String saveOrUpdate(Product product) { try { SolrInputDocument doc = new SolrInputDocument(); doc.setField("id", product.getId()); doc.setField("userId", product.getUserId()); doc.setField("spuNo", product.getSpuNo()); doc.setField("categoryId", product.getCategoryId()); doc.setField("name", product.getName()); doc.setField("type", product.getType()); doc.setField("labels", product.getLabels()); if (product.getPrice() != null) { doc.setField("price", product.getPrice().doubleValue()); } if (product.getFixedPrice() != null) { doc.setField("fixedPrice", product.getFixedPrice().doubleValue()); } if (product.getMinPrice() != null) { doc.setField("minPrice", product.getMinPrice().doubleValue()); } if (product.getMaxPrice() != null) { doc.setField("maxPrice", product.getMaxPrice().doubleValue()); } doc.setField("pics", product.getPics()); doc.setField("initialSales", product.getInitialSales()); doc.setField("realSales", product.getRealSales()); doc.setField("onSale", product.getOnSale()); doc.setField("onSaleTime", product.getOnSaleTime()); doc.setField("isMul", product.getIsMul()); doc.setField("expressTempId", product.getExpressTempId()); doc.setField("stock", product.getStock()); doc.setField("stockType", product.getStockType()); doc.setField("priority", product.getPriority()); doc.setField("fatherCategoryId", product.getFatherCategoryId()); doc.setField("fromLibId", product.getFromLibId()); doc.setField("vipPriceEnable", product.getVipPriceEnable()); doc.setField("createTime", product.getCreateTime()); doc.setField("lastUpdateTime", product.getLastUpdateTime()); doc.setField("isDel", false); Promotion promotion = promotionService.findByProductId(product.getId()); if (null != promotion) { doc.setField("promotionType", promotion.getPromotionType()); } solrClient.add("mall_product", doc); solrClient.commit("mall_product"); return "success"; } catch (Exception e) { e.printStackTrace(); } return "error"; } @Override public String delete(String id) { try { solrClient.deleteById("mall_product", id); solrClient.commit("mall_product"); return "success"; } catch (Exception e) { e.printStackTrace(); } return "error"; } @Override public String deleteAll() { try { solrClient.deleteByQuery("mall_product", "*:*"); solrClient.commit("mall_product"); return "success"; } catch (Exception e) { e.printStackTrace(); } return "error"; } @Override public String getById(String id) { try { SolrDocument document = solrClient.getById("mall_product", id); return document.toString(); } catch (Exception e) { e.printStackTrace(); } return null; } @Override public List&lt;ProductSolrEntity&gt; search(ProductSolrQuery productSolrQuery) { try { SolrQuery params = new SolrQuery(); //查询条件, 这里的 q 对应 下面图片标红的地方 params.setQuery("name:" + productSolrQuery.getName() + " OR " + "spuNo:" + productSolrQuery.getName()); //过滤条件 params.addFilterQuery("userId:" + productSolrQuery.getUserId()); params.addFilterQuery("onSale:T"); if (StringUtils.isNotBlank(productSolrQuery.getFatherCategoryId())) { params.addFilterQuery("fatherCategoryId:" + productSolrQuery.getFatherCategoryId()); } if (StringUtils.isNotBlank(productSolrQuery.getCategoryId())) { params.addFilterQuery("categoryId:" + productSolrQuery.getCategoryId()); } //排序 params.addSort(productSolrQuery.getSortFiled(), productSolrQuery.getSort()); //分页 params.setStart(productSolrQuery.getPage()); params.setRows(productSolrQuery.getPageSize()); QueryResponse queryResponse = solrClient.query("mall_product", params); List&lt;ProductSolrEntity&gt; beans = queryResponse.getBeans(ProductSolrEntity.class); return beans; } catch (Exception e) { e.printStackTrace(); } return null; } @Override public List&lt;String&gt; suggest(String name) { try { SolrQuery query = new SolrQuery(); // 请求到suggest中 query.set("qt", "/suggest"); // 查询的词 query.set("spellcheck.q", name); // 返回数量 query.set("spellcheck.count", "10"); QueryResponse rsp = solrClient.query("mall_product", query); // 上面取结果的代码 // 获取拼写检查的结果集 List&lt;String&gt; list = new ArrayList&lt;&gt;(); SpellCheckResponse re = rsp.getSpellCheckResponse(); if (re != null) { for (SpellCheckResponse.Suggestion s : re.getSuggestions()) { // 获取所有 的检索词 List&lt;String&gt; alternatives = s.getAlternatives(); list.addAll(alternatives); } } return list; } catch (Exception e) { e.printStackTrace(); } return new ArrayList&lt;&gt;(); } }]]></content>
      <categories>
        <category>搜索引擎</category>
      </categories>
      <tags>
        <tag>solr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[客服系统一]]></title>
    <url>%2F2018%2F10%2F12%2F%E5%AE%A2%E6%9C%8D%E7%B3%BB%E7%BB%9F%E4%B8%80%2F</url>
    <content type="text"><![CDATA[一、技术栈选择Netty使用netty做服务器Socket通信框架。 MongoDB使用MongoDB做用户的会话消息存储 二、Netty介绍什么是Netty？ Netty 是一个利用 Java 的高级网络的能力，隐藏其背后的复杂性而提供一个易于使用的 API 的客户端/服务器框架。Netty 是一个广泛使用的 Java 网络编程框架,Netty 在 2011 年获得了Duke’s Choice Award。它活跃和成长于用户社区，像大型公司 Facebook 和 Instagram 以及流行 开源项目如 Infinispan, HornetQ, Vert.x, Apache Cassandra 和 Elasticsearch 等，都利用其强大的对于网络抽象的核心代码。 为什么Netty受欢迎？并发高Netty是一款基于NIO（Nonblocking I/O，非阻塞IO）开发的网络通信框架，对比于BIO（Blocking I/O，阻塞IO），他的并发性能得到了很大提高，两张图让你了解BIO和NIO的区别： 从这两图可以看出，NIO的单线程能处理连接的数量比BIO要高出很多，而为什么单线程能处理更多的连接呢？原因就是图二中出现的Selector。当一个连接建立之后，他有两个步骤要做，第一步是接收完客户端发过来的全部数据，第二步是服务端处理完请求业务之后返回response给客户端。NIO和BIO的区别主要是在第一步。在BIO中，等待客户端发数据这个过程是阻塞的，这样就造成了一个线程只能处理一个请求的情况，而机器能支持的最大线程数是有限的，这就是为什么BIO不能支持高并发的原因。而NIO中，当一个Socket建立好之后，Thread并不会阻塞去接受这个Socket，而是将这个请求交给Selector，Selector会不断的去遍历所有的Socket，一旦有一个Socket建立完成，他会通知Thread，然后Thread处理完数据再返回给客户端——这个过程是阻塞的，这样就能让一个Thread处理更多的请求了。下面两张图是基于BIO的处理流程和netty的处理流程，辅助你理解两种方式的差别： 除了BIO和NIO之外，还有一些其他的IO模型，下面这张图就表示了五种IO模型的处理流程： BIO，同步阻塞IO，阻塞整个步骤，如果连接少，他的延迟是最低的，因为一个线程只处理一个连接，适用于少连接且延迟低的场景，比如说数据库连接。 NIO，同步非阻塞IO，阻塞业务处理但不阻塞数据接收，适用于高并发且处理简单的场景，比如聊天软件。 多路复用IO，他的两个步骤处理是分开的，也就是说，一个连接可能他的数据接收是线程a完成的，数据处理是线程b完成的，他比BIO能处理更多请求，但是比不上NIO，但是他的处理性能又比BIO更差，因为一个连接他需要两次system call，而BIO只需要一次，所以这种IO模型应用的不多。 信号驱动IO，这种IO模型主要用在嵌入式开发，不参与讨论。 异步IO，他的数据请求和数据处理都是异步的，数据请求一次返回一次，适用于长连接的业务场景。 以上摘自Linux IO模式及 select、poll、epoll详解 传输快Netty的传输快其实也是依赖了NIO的一个特性——零拷贝。我们知道，Java的内存有堆内存、栈内存和字符串常量池等等，其中堆内存是占用内存空间最大的一块，也是Java对象存放的地方，一般我们的数据如果需要从IO读取到堆内存，中间需要经过Socket缓冲区，也就是说一个数据会被拷贝两次才能到达他的的终点，如果数据量大，就会造成不必要的资源浪费。Netty针对这种情况，使用了NIO中的另一大特性——零拷贝，当他需要接收数据的时候，他会在堆内存之外开辟一块内存，数据就直接从IO读到了那块内存中去，在netty里面通过ByteBuf可以直接对这些数据进行直接操作，从而加快了传输速度。下两图就介绍了两种拷贝方式的区别，摘自摘自Linux 中的零拷贝技术，第 1 部分 上文介绍的ByteBuf是Netty的一个重要概念，他是netty数据处理的容器，也是Netty封装好的一个重要体现，将在下一部分做详细介绍。 封装好要说Netty为什么封装好，这种用文字是说不清的，直接上代码： 阻塞I/O 1234567891011121314151617181920212223242526272829303132333435public class PlainOioServer &#123; public void serve(int port) throws IOException &#123; final ServerSocket socket = new ServerSocket(port); //1 try &#123; for (;;) &#123; final Socket clientSocket = socket.accept(); //2 System.out.println("Accepted connection from " + clientSocket); new Thread(new Runnable() &#123; //3 @Override public void run() &#123; OutputStream out; try &#123; out = clientSocket.getOutputStream(); out.write("Hi!\r\n".getBytes(Charset.forName("UTF-8"))); //4 out.flush(); clientSocket.close(); //5 &#125; catch (IOException e) &#123; e.printStackTrace(); try &#123; clientSocket.close(); &#125; catch (IOException ex) &#123; // ignore on close &#125; &#125; &#125; &#125;).start(); //6 &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 非阻塞IO 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class PlainNioServer &#123; public void serve(int port) throws IOException &#123; ServerSocketChannel serverChannel = ServerSocketChannel.open(); serverChannel.configureBlocking(false); ServerSocket ss = serverChannel.socket(); InetSocketAddress address = new InetSocketAddress(port); ss.bind(address); //1 Selector selector = Selector.open(); //2 serverChannel.register(selector, SelectionKey.OP_ACCEPT); //3 final ByteBuffer msg = ByteBuffer.wrap("Hi!\r\n".getBytes()); for (;;) &#123; try &#123; selector.select(); //4 &#125; catch (IOException ex) &#123; ex.printStackTrace(); // handle exception break; &#125; Set&lt;SelectionKey&gt; readyKeys = selector.selectedKeys(); //5 Iterator&lt;SelectionKey&gt; iterator = readyKeys.iterator(); while (iterator.hasNext()) &#123; SelectionKey key = iterator.next(); iterator.remove(); try &#123; if (key.isAcceptable()) &#123; //6 ServerSocketChannel server = (ServerSocketChannel)key.channel(); SocketChannel client = server.accept(); client.configureBlocking(false); client.register(selector, SelectionKey.OP_WRITE | SelectionKey.OP_READ, msg.duplicate()); //7 System.out.println( "Accepted connection from " + client); &#125; if (key.isWritable()) &#123; //8 SocketChannel client = (SocketChannel)key.channel(); ByteBuffer buffer = (ByteBuffer)key.attachment(); while (buffer.hasRemaining()) &#123; if (client.write(buffer) == 0) &#123; //9 break; &#125; &#125; client.close(); //10 &#125; &#125; catch (IOException ex) &#123; key.cancel(); try &#123; key.channel().close(); &#125; catch (IOException cex) &#123; // 在关闭时忽略 &#125; &#125; &#125; &#125; &#125;&#125; Netty 12345678910111213141516171819202122232425262728293031public class NettyOioServer &#123; public void server(int port) throws Exception &#123; final ByteBuf buf = Unpooled.unreleasableBuffer( Unpooled.copiedBuffer("Hi!\r\n", Charset.forName("UTF-8"))); EventLoopGroup group = new OioEventLoopGroup(); try &#123; ServerBootstrap b = new ServerBootstrap(); //1 b.group(group) //2 .channel(OioServerSocketChannel.class) .localAddress(new InetSocketAddress(port)) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123;//3 @Override public void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(new ChannelInboundHandlerAdapter() &#123; //4 @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; ctx.writeAndFlush(buf.duplicate()).addListener(ChannelFutureListener.CLOSE);//5 &#125; &#125;); &#125; &#125;); ChannelFuture f = b.bind().sync(); //6 f.channel().closeFuture().sync(); &#125; finally &#123; group.shutdownGracefully().sync(); //7 &#125; &#125;&#125; 从代码量上来看，Netty就已经秒杀传统Socket编程了，但是这一部分博大精深，仅仅贴几个代码岂能说明问题，在这里给大家介绍一下Netty的一些重要概念，让大家更理解Netty。 Channel数据传输流，与channel相关的概念有以下四个，上一张图让你了解netty里面的Channel。 Channel，表示一个连接，可以理解为每一个请求，就是一个Channel。 ChannelHandler，核心处理业务就在这里，用于处理业务请求。 ChannelHandlerContext，用于传输业务数据。 ChannelPipeline，用于保存处理过程需要用到的ChannelHandler和ChannelHandlerContext. ByteBufByteBuf是一个存储字节的容器，最大特点就是使用方便，它既有自己的读索引和写索引，方便你对整段字节缓存进行读写，也支持get/set，方便你对其中每一个字节进行读写，他的数据结构如下图所示：他有三种使用模式： Heap Buffer 堆缓冲区堆缓冲区是ByteBuf最常用的模式，他将数据存储在堆空间。 Direct Buffer 直接缓冲区直接缓冲区是ByteBuf的另外一种常用模式，他的内存分配都不发生在堆，jdk1.4引入的nio的ByteBuffer类允许jvm通过本地方法调用分配内存，这样做有两个好处 通过免去中间交换的内存拷贝, 提升IO处理速度; 直接缓冲区的内容可以驻留在垃圾回收扫描的堆区以外。 DirectBuffer 在 -XX:MaxDirectMemorySize=xxM大小限制下, 使用 Heap 之外的内存, GC对此”无能为力”,也就意味着规避了在高负载下频繁的GC过程对应用线程的中断影响. Composite Buffer 复合缓冲区复合缓冲区相当于多个不同ByteBuf的视图，这是netty提供的，jdk不提供这样的功能。除此之外，他还提供一大堆api方便你使用，在这里我就不一一列出了，具体参见ByteBuf字节缓存。 CodecNetty中的编码/解码器，通过他你能完成字节与pojo、pojo与pojo的相互转换，从而达到自定义协议的目的。在Netty里面最有名的就是HttpRequestDecoder和HttpResponseEncoder了。 三、MongoDB介绍简介MongoDB是用C++语言编写的非关系型数据库。特点是高性能、易部署、易使用，存储数据十分方便，主要特性有： 面向集合存储，易于存储对象类型的数据 模式自由 支持动态查询 支持完全索引，包含内部对象 支持复制和故障恢复 使用高效的二进制数据存储，包括大型对象 文件存储格式为BSON(一种JSON的扩展) MongoDB和关系数据库的对比 对比项 mongoDB mysql oracle 表 集合 二维表table 表的一行数据 文档document 一条记录recoder 表字段 键key 字段filed 字段值 值value 值value 主外键 无 PK FK 灵活度扩展性 极高 差 MongoDB基本概念 文档(document)是MongoDB中数据的基本单元，非常类似于关系型数据库系统中的行(但是比行要复杂的多)。 集合(collection)就是一组文档，如果说MongoDB中的文档类似于关系型数据库中的行，那么集合就如同表。 MongoDB的单个计算机可以容纳多个独立的数据库，每一个数据库都有自己的集合和权限。 MongoDB自带简洁但功能强大的JavaScript shell，这个工具对于管理MongoDB实例和操作数据作用非常大。 每一个文档都有一个特殊的键”_id”,它在文档所处的集合中是唯一的，相当于关系数据库中的表的主键。 MongoDB数据类型 数据类型 描述 举例 null 表示空值或者未定义的对象 {“x”:null} 布尔值 真或者假：true或者false {“x”:true} 32位整数 32位整数。shell是不支持该类型的，shell中默认会转换成64位浮点数 64位整数 64位整数。shell是不支持该类型的，shell中默认会转换成64位浮点数 64位浮点数 64位浮点数。shell中的数字就是这一种类型 {“x”：3.14，”y”：3} 字符串 UTF-8字符串 {“foo”:”bar”} 符号 shell不支持，shell会将数据库中的符号类型的数据自动转换成字符串 对象id 文档的12字节的唯一id {“id”: ObjectId()} 日期 从标准纪元开始的毫秒数 {“date”:new Date()} 正则表达式 文档中可以包含正则表达式，遵循JavaScript的语法 {“foo”:/foobar/i} 代码 文档中可以包含JavaScript代码 {“x”：function() {}} 未定义 undefined {“x”：undefined} 数组 值的集合或者列表 {“arr”: [“a”,”b”]} 内嵌文档 文档可以作为文档中某个key的value {“x”:{“foo”:”bar”}}]]></content>
      <categories>
        <category>客服系统</category>
      </categories>
      <tags>
        <tag>im</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[极光推送的配置与使用]]></title>
    <url>%2F2018%2F09%2F30%2F%E6%9E%81%E5%85%89%E6%8E%A8%E9%80%81%E7%9A%84%E9%85%8D%E7%BD%AE%E4%B8%8E%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[一、进入极光官网创建自己的应用 进入极光推送官网注册账号 登录之后进入如下页面 创建应用包名和图标随便写记住你的AppKey 和Master Secret 后面会用到 进入推送设置 设置一个安卓的应用下载到自己手机,配置一个别名 二、安装 maven方式将下边的依赖条件放到你项目的 maven pom.xml 文件里。 12345&lt;dependency&gt; &lt;groupId&gt;cn.jpush.api&lt;/groupId&gt; &lt;artifactId&gt;jpush-client&lt;/artifactId&gt; &lt;version&gt;3.3.7&lt;/version&gt;&lt;/dependency&gt; jar 包方式请到 Release页面下载相应版本的发布包。 依赖包 slf4j / log4j (Logger) gson(Google JSON Utils) 其中 slf4j 可以与 logback, log4j, commons-logging 等日志框架一起工作，可根据你的需要配置使用。 如果使用 Maven 构建项目，则需要在你的项目 pom.xml 里增加： 123456789101112131415161718192021222324252627282930313233&lt;dependency&gt; &lt;groupId&gt;cn.jpush.api&lt;/groupId&gt; &lt;artifactId&gt;jiguang-common&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.netty&lt;/groupId&gt; &lt;artifactId&gt;netty-all&lt;/artifactId&gt; &lt;version&gt;4.1.6.Final&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt; &lt;artifactId&gt;gson&lt;/artifactId&gt; &lt;version&gt;2.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.7&lt;/version&gt; &lt;/dependency&gt; &lt;!-- For log4j --&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.7&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt; &lt;/dependency&gt; 如果不使用 Maven 构建项目，则项目 libs/ 目录下有依赖的 jar 可复制到你的项目里去。 三、构建推送对象二话不说先上代码1234567891011121314151617181920212223242526272829public static PushPayload buildPushObject_android_ios_alias_alert(String alias, String alert, String type) &#123; return PushPayload.newBuilder() //设置推送平台 安卓和ios .setPlatform(Platform.android_ios()) //设置推送目标，别名是alias的设备 .setAudience(Audience.alias(alias)) //设置通知内容 .setNotification(Notification.newBuilder() .addPlatformNotification(AndroidNotification.newBuilder() .addExtra("type", type) .setAlert(alert) .build()) .addPlatformNotification(IosNotification.newBuilder() .addExtra("type", type) .setAlert(alert) .build()) .build()) //设置推送环境 .setOptions(Options.newBuilder() .setApnsProduction(false)//true-推送生产环境 false-推送开发环境（测试使用参数） .setTimeToLive(3600)//消息在JPush服务器的失效时间（测试使用参数） .build()) //设置推送内容 .setMessage(Message.newBuilder() .setMsgContent(alert) .addExtra("type", type) .build()) .build(); &#125; 四、极光推送方法12345678910111213141516171819public static PushResult push(String alias, String alert, String type) &#123; ClientConfig clientConfig = ClientConfig.getInstance(); //masterSecret和appKey 分别填上自己申请的 JPushClient jpushClient = new JPushClient(masterSecret, appKey, null, clientConfig); PushPayload payload = buildPushObject_android_ios_alias_alert(alias, alert, type); try &#123; return jpushClient.sendPush(payload); &#125; catch (APIConnectionException e) &#123; log.error("Connection error. Should retry later. ", e); return null; &#125; catch (APIRequestException e) &#123; log.error("Error response from JPush server. Should review and fix it. ", e); log.info("HTTP Status: " + e.getStatus()); log.info("Error Code: " + e.getErrorCode()); log.info("Error Message: " + e.getErrorMessage()); log.info("Msg ID: " + e.getMsgId()); return null; &#125; &#125; 五、测试随便写个测试的推送方法12345678910public static void main(String[] args) &#123; String alias = "****";//声明别名 之前在你手机设置的别名 log.info("对别名" + alias + "的用户推送信息"); PushResult result = push(String.valueOf(alias), ALERT, "***"); if (result != null &amp;&amp; result.isResultOK()) &#123; log.info("针对别名" + alias + "的信息推送成功！"); &#125; else &#123; log.info("针对别名" + alias + "的信息推送失败！"); &#125; &#125; 出现这个说明推送成功了 手机也会收到你推送的消息 六、相关异常的codeHTTP 返回码为 200 时，是业务相关的错误。 错误码 错误描述 0 调用成功 10 系统内部错误 1001 只支持 HTTP Post 方法，不支持 Get 方法 1002 缺少了必须的参数 1003 参数值不合法 1004 verification_code 验证失败 1005 消息体太大 1007 receiver_value 参数 非法 1008 appkey参数非法 1010 msg_content 不合法 1011 没有满足条件的推送目标 1012 iOS 不支持推送自定义消息。只有 Android 支持推送自定义消息。 1013 content-type 只支持 application/x-www-form-urlencoded 1014 消息内容包含敏感词汇。 1030 内部服务超时。稍后重试。 返回 1011 时：如果群发：则此应用还没有一个客户端用户注册。请检查 SDK 集成是否正常。如果是推送给某别名或者标签：则此别名或者标签还没有在任何客户端 SDK 提交设置成功。]]></content>
      <categories>
        <category>工具类</category>
      </categories>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建hexo博客]]></title>
    <url>%2F2018%2F09%2F27%2F%E6%90%AD%E5%BB%BAhexo%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[一、安装Nodejs下载地址: https://nodejs.org/ 二、安装Git for windows下载地址: https://git-for-windows.github.io/一路Next安装就好 三、安装Hexo打开Git Bash，执行命令: npm install hexo-cli -g 如果不行就先执行: npm --registry http://registry.cnpmjs.org info underscore 再执行: npm install hexo-cli -g 然后执行: hexo -v 如果安装成功的话会出现如下图: 四、新建一个blog项目文件夹比如：在D盘新建一 个D：/myblog放置blog项目。点击鼠标右键，选择Git Bash Here。 然后依次执行以下命令: 1234hexo initnpm install hexo ghexo s 在本地浏览器上输入地址:http://localhost:4000/ 就可以看到如下图画面了 五、配置Git 登录github创建仓库创建一个repo，名称为yourname.github.io, 其中yourname是你的github名称，按照这个规则创建才有用哦 配置本地github账户信息打开gitbash 12git config --global user.name &quot;&lt;你的name&gt;&quot;git config --global user.email &quot;&lt;你的email&gt;&quot; 创建ssh在gitbash中输入:ssh-keygen -t rsa -C &quot;youremail@example.com 按提示找id_rsa.pub文件，可以用记事本打开，复制全部内容。在浏览器中打开 https://github.com ，登录并打开settings，在SSH and GPG keys下New SSH Key，title随便填写，key就粘贴复制id_rsa.pub文件的内容。在gitbash中执行以下命令验证是否添加成功:ssh -T git@github.com 六、更改hexo配置 用编辑器打开你的blog项目，修改_config.yml文件的一些配置(冒号之后都是有一个半角空格的): 1234deploy: type: git repo: https://github.com/YourgithubName/YourgithubName.github.io.git branch: master 回到gitbash中，进入你的blog目录，分别执行以下命令: 123hexo cleanhexo generatehexo server 注：hexo 3.0把服务器独立成个别模块，需要单独安装:npm i hexo-server 打开浏览器输入：http://localhost:4000 接着你就可以遇见天使的微笑了~ 七、上传到github 先安装一波：npm install hexo-deployer-git --save（这样才能将你写好的文章部署到github服务器上并让别人浏览到） 执行命令(建议每次都按照如下步骤部署):123hexo cleanhexo generatehexo deploy 注意deploy的过程中要输入你的username及passward，最后出现Deploy done:git 说明上传成功了最后浏览器输入http://yourgithubname.github.io就能看到你的博客了！ 八、绑定个人域名 第一步购买域名：随便在哪个网站买一个就好了，我是在阿里云购买的jingxu.top,六块钱一年不贵。 第二步添加CNAME：在项目的source文件夹下新建一个名为CNAME的文件，在里面添加你购买的域名，比如我添加的是jingxu.top，只能添加一个。 到DNS中添加一条记录： 接着再次部署一下，用你购买的域名打开，就可以看到你的博客啦~ 如果没有的话 在setting 往下拉 会看见如果没有Custom domain可能是你CNAME名字写错了，全部大写CNAME 不是 GNAME]]></content>
      <categories>
        <category>hexo博客</category>
      </categories>
      <tags>
        <tag>博客环境搭建</tag>
      </tags>
  </entry>
</search>
